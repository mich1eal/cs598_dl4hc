%
% File acl2021.tex
%
%% Based on the style files for EMNLP 2020, which were
%% Based on the style files for ACL 2020, which were
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2021}
\usepackage{times}
\usepackage{latexsym}
\usepackage{usebib}
\usepackage{hyperref}
\bibinput{acl2021}
\renewcommand{\UrlFont}{\ttfamily\small}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

% Content lightly modified from original work by Jesse Dodge and Noah Smith


\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{CS598 DL4HC Reproducibility Project Proposal}

\author{Michael Miller and Kurt Tuohy\\
  \texttt{\{msmille3, ktuohy\}@illinois.edu}
  \\[2em]
  Presentation link: TODO\url{} \\
  Code repository: \url{https://github.com/mich1eal/cs598_dl4hc} \citep{cs598_repo}}

\begin{document}
\maketitle

% All sections are mandatory.
% Keep in mind that your page limit is 8, excluding references.
% For specific grading rubrics, please see the project instruction.

\section{Introduction}

The majority of text written by patients in psychological care is only interpreted by human therapists \citep{burger_2021}. In their paper, \citeauthor{burger_2021} begin to explore the potential of NLP as an aid to cognitive therapy. They seek to determine whether NLP techniques can reliably classify patient writing.

A useful cognitive therapy exercise involves having patients identify the thoughts that underlie their reactions to potentially distressing situations. The authors recruited online participants to participate in this exercise, manually classified the participants' thought records into schemas, then trained models to perform the same classification.

``Schemas" refer to core beliefs that function as the infrastructure to one's views of the world. A standard set of schemas defined by \citeauthor{millings_2015} was used.

\citeauthor{burger_2021} establish a baseline for applying NLP to cognitive therapy. They intend for future researchers to leverage their paper and data to continue to explore applications of NLP in this field. In this paper, we verify the results of \citeauthor{burger_2021}, and implement several design modifications as further studies. 

\section{Scope of reproducibility}

In this paper, we test the central hypothesis of \citeauthor{burger_2021}. We verify that schemas can be automatically predicted from patient thought records, and that this can be done with accuracy greater than random chance.

We re-implement four models used in the paper:
\begin{enumerate}
    \item K-nearest neighbors (classifier and regression algorithms)
    \item Support vector machines (classifier and regression algorithms)
    \item Multi-label LSTM, i.e. a single model to output predictions for all schemas (regression algorithm)
    \item Per-schema LSTMs, i.e. one LSTM for each schema (classification algorithm)
\end{enumerate}

In order to generate comparable results, we adopt a number of the design choices of \citeauthor{burger_2021}. We use pre-trained GLoVE embeddings \citep{pennington_2014} in all of the above models. 

We also measure model performance using Spearman's rank-order correlation between predictions and ground truth. 

\subsection{Addressed claims from the original paper}
Because of our similar design, we expect to obtain similar results to \citeauthor{burger_2021}. In particular, we will verify the following three claims:
\begin{enumerate}
    \item Per-schema LSTMs outperform other tested models on all schemas except ``Power and Control" and ``Meta-Cognition".
    \item Each regression model outperforms its corresponding classifier on the ``Power and Control" schema.
    \item Each classifier outperforms the corresponding regression model on the ``Health" schema.
\end{enumerate}

\section{Methodology}
\subsection{Model descriptions}
\subsubsection{K-nearest Neighbors}

\subsubsection{Support Vector Machine}
Support vector machines (classifier and regression algorithms)

\subsubsection{Multi-label LSTM}
Multi-label LSTM, i.e. a single model to output predictions for all schemas (regression algorithm)
A single-direction Long Short Term Memory (LSTM) Recurrant Neural Network (RNN) was implemented with 100 parameters in its hidden state. The model also included an embedding layer (using GLoVE embeddings). A single fully connected layer was used to reduced the LSTM output to the 9 classes. ReLU was used as an activate function. A softmax function was applied to view the outputs as probabilities. 

The model was trained using cross entropy loss with the Adam optimizer. 

\subsubsection{Per-schema LSTMs}
A total of nine models were trained, one for each schema. Model architecture is similar to the LSTM defined above, with some differences. Namely: asdlkjfa. 

\subsection{Data descriptions}
The data used in the original paper is available online \citep{burger_2021_data}. The data is referenced at multiple phases: 
\begin{enumerate}
    \item Fully raw - data directly pulled from the authors' survey. This data contains sensitive information and is not published. 
    \item Anonymized raw - data that has been anonymized. This data is otherwise in its original form
    \label{anon_raw}
    \item Prepossessed - data has been normalized, spell-checked, stripped of common words, and split into test/validation/train sets. 
    \label{preprocess}
\end{enumerate}

Dataset \ref{anon_raw} was used to recreate authors' original results. By taking advantage of the authors' preprocessing, fewer confounding variables will exist in the output. 

Dataset \ref{preprocess} was used for additional studies. It is hypothesized that keeping common words in the data may improve performance, especially when using larger, more expressive models. 

\subsection{Hyperparameters}
For their RNN models, \citeauthor{burger_2021} conducted a large parameter sweep to optimize hyperparameters. It is not our intent to duplicate this effort. We will instead focus on verifying the authors' result with optimal hyperparameters. As a result, an additional hyperparameter sweep will not be conducted. 

For additional studies, hyperparameters were selected using a hyperparameter sweep. 

\subsection{Implementation}
The authors' code is available in its entirety online \citep{burger_2021_data}. While the authors' code was referenced, the code for this paper is original. 

The codebase of this paper is available online \citep{cs598_repo}. All scripting code is written in Python. A number of python modules were used, in particular, we used pyTorch as our primary deep learning module. A full list of Python module dependencies in available in the repository. 

Code was executed on machines running Windows 10 using Python 3.10. GPUs were deemed unnecessary for training (see \nameref{comp_reqs}). 

During implementation, a number of documentation sources were reviewed. These include documentation for individual Python modules, online tutorials and forums, and course material from this and other classes.  

\subsection{Computational requirements}
\label{comp_reqs}

In our proposal, we stated that a relatively small dataset (5747 utterances), and relatively simple models were used. 
The largest time requirements of \citet{burger_2021} exist because they trained hundreds of RNN models to decrease the role of chance in the final results. We planned to reduce the number of models trained while still confirming key results, if computation time became prohibitive. As such, we planned execution using standard computers without access to GPUs, TPUs, or computational clusters. 

In general, this approach appears to be succeeding. Training an individual RNN takes on the order of 10 minutes of runtime on a conventional CPU. This performance is satisfactory for development and validation. Final model evaluation may be run overnight but is not expected to become an inhibiting factor. 

As models are finalized, code will be added to capture more accurate metrics about runtime to be documented in the final report. 

\section{Results}
The authors' two RNN models have been implemented. The initial implementation matches within 5. Work will be done to make the results even better. 

Additionally, the authors other models will be implemented and added to the table below. 

Our further research will be added in \nameref{further_studies}. 

\subsection{Hypothesis 1: automatic schema prediction}

Author score, our score 

\subsection{Further studies}
\label{further_studies}

We plan to conduct the following additional research, to be further defined and finalized in with our final submission. 

Scenario 1. We us the authors RNN models with the following modifications. We leave all words in during preprocessing. 

Scenario 2. Scenario 1 but we also use BERT 

Scores

\section{Discussion}
TODO
\subsection{What was easy}
TODO
\subsection{What was difficult}
TODO
\subsection{Recommendations for reproducibility}
TODO
\section{Communication with original authors}
TODO
\bibliographystyle{acl_natbib}
\bibliography{acl2021}

%\appendix


\end{document}
