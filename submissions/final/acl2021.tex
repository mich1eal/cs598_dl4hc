%
% File acl2021.tex
%
%% Based on the style files for EMNLP 2020, which were
%% Based on the style files for ACL 2020, which were
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2021}
\usepackage{float}
\usepackage{times}
\usepackage{latexsym}
\usepackage{usebib}
\usepackage{hyperref}
\usepackage{booktabs}
\bibinput{acl2021}
\renewcommand{\UrlFont}{\ttfamily\small}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

% Content lightly modified from original work by Jesse Dodge and Noah Smith


\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{CS598 DL4HC Reproducibility Project Proposal}

\author{Michael Miller and Kurt Tuohy\\
  \texttt{\{msmille3, ktuohy\}@illinois.edu}
  \\[2em]
  Group ID: 127\\
  Paper ID: 117, Difficulty: easy\\
  Presentation link: TODO\url{} \\
  Code repository: \url{https://github.com/mich1eal/cs598_dl4hc} \citep{cs598_repo}}

\begin{document}
\maketitle

\author{Author1 and Author2 \\
  \texttt{\{author1, author2\}@illinois.edu}
  \\[2em]
  Group ID: X\\
  Paper ID: X, Difficulty: [easy/hard]\\
  Presentation link: \url{https://www.youtube.com} \\
  Code link: \url{https://www.github.com}} 

\begin{document}
\maketitle

% All sections are mandatory.
% Keep in mind that your page limit is 8, excluding references.
% For specific grading rubrics, please see the project instruction.

\section{Introduction}

The majority of text written by patients in psychological care is only interpreted by human therapists \citep{burger_2021}. In their paper, \citeauthor{burger_2021} begin to explore the potential of NLP as an aid to cognitive therapy. They seek to determine whether NLP techniques can reliably classify patient writing.

A useful cognitive therapy exercise involves having patients identify the thoughts that underlie their reactions to potentially distressing situations. The authors attempt to classify a set of these written thoughts into schemas. ``Schemas" refer to core beliefs that function as the infrastructure to one's views of the world. A standard set of schemas defined by \citeauthor{millings_2015} was used.

\citeauthor{burger_2021} establish a baseline for applying NLP to cognitive therapy. They intend for future researchers to leverage their paper and data to continue to explore applications of NLP in this field. In this paper, we verify the results of \citeauthor{burger_2021}, and implement several design modifications as further studies. 

\section{Scope of reproducibility}

In this paper, we test the central hypothesis of \citeauthor{burger_2021} We verify that schemas can be automatically predicted from patient thought records, and that this can be done with accuracy greater than random chance.

We re-implement four models used in the paper:
\begin{enumerate}
    \item k-nearest neighbors (classifier and regression algorithms)
    \item Support vector machines (classifier and regression algorithms)
    \item Multi-label LSTM, i.e. a single model to output predictions for all schemas (regression algorithm)
    \item Per-schema LSTMs, i.e. one LSTM for each schema (classification algorithm)
\end{enumerate}

In order to generate comparable results, we adopt a number of the design choices of \citeauthor{burger_2021}. We use pre-trained GLoVe embeddings \citep{pennington_2014} in all of the above models. 

We also measure model performance using Spearman's rank-order correlation between predictions and ground truth. 

\subsection{Addressed claims from the original paper}
\label{claims}
Because of our similar design, we expect to obtain similar results to \citeauthor{burger_2021}. In particular, we will verify the following three claims:
\begin{enumerate}
    \item Per-schema LSTMs outperform other tested models on all schemas except ``Power and Control" and ``Meta-Cognition".
    \item Each regression model outperforms its corresponding classifier on the ``Power and Control" schema.
    \item Each classifier outperforms the corresponding regression model on the ``Health" schema.
\end{enumerate}

\section{Methodology}
\subsection{Model descriptions}
\subsubsection{K-nearest Neighbors}
\citeauthor{burger_2021} generated kNN models for selected values of k from 2 to 100. These models used the cosine distance between thought-record embeddings as the distance metric.

To generate predictions, the classification models computed the statistical mode of the k nearest neighbors' labels. The regression models used the average of the nearest neighbors' labels.

k=4 gave the best results for classification models, and k=5 was best for regression models.

\subsubsection{Support Vector Machine}
\citeauthor{burger_2021} implemented both Support Vector Machine Classifier (SVM-C) and Regression (SVM-R) models. For both implementations, a separate classifier was created for each schema. Each SVM outputs a value for its schema, and all nine were evaluated together using Spearman's correlation coefficient. \citeauthor{burger_2021} selected Radial Basis Function (RBF) as the most effective kernel. RBF has one trainable parameter per input dimension, plus the SVM's bias term. Utterance-level GLoVE embeddings with 100 dimensions were used as inputs. As a result, nine SVM models have a total of 909 trainable parameters. 

\subsubsection{Multi-label LSTM}
\label{multi_lstm}
A single model was trained to output predictions for all nine schemas.

For this model, the researchers obtained the best performance from a bidirectional Long Short Term Memory (LSTM) Recurrent Neural Network (RNN) with 100 parameters in its hidden state.

The model also included an embedding layer, which used GLoVE embeddings on a per-word basis. Following the LSTM, a 10\% dropout layer fed a single fully connected layer, which reduced the output to the 9 classes. Finally, a sigmoid activation function output per-class probabilities. 

The model was trained using cross entropy loss with the Adam optimizer and a default learning rate of 0.001.

The number of trainable parameters was 162,609. The embedding involved 262,400 non-trainable parameters, for a total of 425,009.

\subsubsection{Per-schema LSTMs}
A total of nine models were trained, one for each schema. Model architecture is similar to the LSTM defined above, with the following differences:

Instead of 9 output classes, each model used 4. This matched the researcher's 0-3 scale for rating thought records against schemas, where 0 meant "no correspondence with schema" and 3 meant "complete correspondence with schema".

Instead of a sigmoid activation, these models used softmax, since points on the rating scale are mutually exclusive.

Each per-schema model had 161,604 trainable parameters, with the same number of non-trainable parameters as the multi-label model. The total was 424,004.

\subsubsection{Note on embeddings}
The researchers created custom per-thought-record embeddings for the kNN and SVM models. In contrast, the RNN models used basic per-word GLoVe embeddings as weights.

For the custom embeddings, each word's GLoVe embedding was multiplied by the word's TF-IDF weight. The resulting vector was divided by its 2-norm. Then, for each thought record, the applicable word vectors were summed and averaged. This gave one embedding per thought record.

\subsection{Data descriptions}
The data used in the original paper is available online \citep{burger_2021_data}. The data is referenced at multiple phases: 
\begin{enumerate}
    \item Fully raw - data directly pulled from the authors' survey. This data contains sensitive information and is not published. 
    \item Anonymized raw - data that has been anonymized. This data is otherwise in its original form. It includes participant IDs, the situational scenarios that participants responded to, the thought records themselves, other grouping information, and the ground-truth correspondence scores for each schema on a 0-3 scale.
    \label{anon_raw}
    \item Preprocessed - thought records have been normalized, spell-checked, stripped of common words, and split into test/validation/train sets. Thought records have also been decoupled from contextual information like participant IDs and situational scenarios.
    \label{preprocess}
\end{enumerate}

Dataset \ref{preprocess} was used to recreate authors' original results. By taking advantage of the authors' preprocessing, fewer confounding variables will exist in the output. 

Dataset \ref{anon_raw} will be used for additional ablations. It is hypothesized that keeping common words in the data may improve performance, especially when using larger, more expressive models. 

\subsection{Hyperparameters}
For their RNN models, \citeauthor{burger_2021} conducted a large parameter sweep to optimize hyperparameters. It is not our intent to duplicate this effort. We will instead focus on verifying the authors' result using their optimal hyperparameters.

For our ablations, we intend to perform a hyperparameter sweep. 



\subsection{Implementation}
The authors' code is available in its entirety online \citep{burger_2021_data}. While the authors' code was referenced, the code for this paper is original. 

The codebase of this paper is available online \citep{cs598_repo}. All scripting code is written in Python. A number of python modules were used, in particular, we used pyTorch as our primary deep learning module. A full list of Python module dependencies in available in the repository. 

Code was executed on machines running Windows 10 using Python 3.10. GPUs were deemed unnecessary for training (see \nameref{comp_reqs}). 

During implementation, a number of documentation sources were reviewed. These include documentation for individual Python modules, online tutorials and forums, and course material from this and other classes.  

\subsection{Computational requirements}
\label{comp_reqs}

In our proposal, we stated that a relatively small dataset (5747 thought records) and relatively simple models were used. 
The largest time requirements of \citet{burger_2021} exist because they trained hundreds of RNN models to decrease the role of chance in the final results. We planned to reduce the number of models trained while still confirming key results, if computation time became prohibitive. As such, we planned execution using standard computers without access to GPUs, TPUs, or computational clusters. 

In general, this approach appears to be succeeding. Training an individual RNN takes 7-10 minutes on a conventional CPU. This performance is satisfactory for development and validation. Final model evaluation may be run overnight but is not expected to become an inhibiting factor. 

The paper's original Jupyter notebook tracked clock time but not CPU usage. Each of their RNNs required about 2.6 minutes to train, which is on the same order of magnitude as our training times.

The script to run our single draft model consumes about 570 MB of RAM, which includes GLoVe embeddings. In comparison, the original notebook consumes about 834 MB before training any models.

Based on partial reruns of the paper's original Jupyter notebook, each additional RNN should require about 26 MB. The kNN and SVM models should be smaller yet.

As models are finalized, code will be added to capture more accurate metrics about runtime to be documented in the final report. 

\section{Results}
The paper's original model types were all reimplemented in pyTorch. Following are the models' point estimates of Spearman correlation coefficients for predicted schemas vs. ground-truth values. We compare them to the original paper's 95\% confidence intervals.

Additional work beyond the original paper's scope is presented in \nameref{further_studies}.

\subsection{Model Results Comparison}
\subsubsection{k-Nearest Neighbors}

Our kNN models are well-behaved, with almost all r values falling within paper's the 95\% confidence bounds. Our classifier originally returned much lower values until we discovered that two utterances with only unknown tokens were being classified as the nearest neighbors of all other utterances, due to the model's cosine distance measure. Results improved dramatically after we removed those two utterances.

\begin{table}[H]
\centering
% you can use l,r,c for left-aligned, right-aligned or centered columns
\begin{tabular}{lcc}
\toprule
\multicolumn{3}{c}{\large kNN Classification} \\
Schema                  &Published          &Reproduced \\ % use ampersands to separate columns and \\ for end of rows
\midrule
Attachment              & 0.55 [0.51, 0.60] & 0.59 \\
Competence              & 0.69 [0.64, 0.73] & 0.64 \\
Global self-eval        & 0.40 [0.33, 0.46] & 0.54 \\
Health                  & 0.74 [0.65, 0.81] & 0.59 \\
Power \& ctrl           & 0.11 [0.02, 0.18] & 0.18 \\
Meta-cognition          & nan [0.00, 1.00] & nan \\
Other people            & 0.28 [0.00, 1.00] & -0.01 \\
Hopelessness            & 0.48 [0.44, 0.55] & 0.59 \\
Other's views           & 0.45 [0.41, 0.51] & 0.42 \\
\bottomrule
\end{tabular}
\caption{Spearman correlation [95\% confidence bounds] for k-nearest neighbors classifier}
\label{tab:knn_classification}
\end{table}

\begin{table}[H]
\centering
% you can use l,r,c for left-aligned, right-aligned or centered columns
\begin{tabular}{lcc}
\toprule
\multicolumn{3}{c}{\large kNN Regression} \\
Schema                  &Published          &Reproduced \\ % use ampersands to separate columns and \\ for end of rows
\midrule
Attachment              & 0.63 [0.59, 0.65] & 0.62 \\
Competence              & 0.66 [0.63, 0.69] & 0.68 \\
Global self-eval        & 0.41 [0.36, 0.46] & 0.47 \\
Health                  & 0.53 [0.44, 0.60] & 0.53 \\
Power \& ctrl           & 0.23 [0.17, 0.27] & 0.23 \\
Meta-cognition          & 0.10 [0.01, 0.20] & 0.02 \\
Other people            & 0.24 [0.17, 0.31] & 0.14 \\
Hopelessness            & 0.51 [0.47, 0.56] & 0.50 \\
Other's views           & 0.46 [0.42, 0.50] & 0.45 \\
\bottomrule
\end{tabular}
\caption{Spearman correlation [95\% confidence bounds] for k-nearest neighbors regressor}
\label{tab:knn_regression}
\end{table}

\subsubsection{Support Vector Machine}

\begin{table}[H]
\centering
\begin{tabular}{lcc}
\toprule
\multicolumn{3}{c}{\large SVC (Classification)} \\
Schema                  &Published          &Reproduced \\
\midrule
Attachment              & 0.65 [0.61, 0.68] & 0.66 \\
Competence              & 0.68 [0.65, 0.72] & 0.68 \\
Global self-eval        & 0.36 [0.31, 0.40] & 0.54 \\
Health                  & 0.73 [0.65, 0.81] & 0.63 \\
Power \& ctrl           & nan [0.00, 1.00] & 0.10 \\
Meta-cognition          & nan [0.00, 1.00] & nan \\
Other people            & nan [0.00, 1.00] & -0.01 \\
Hopelessness            & 0.49 [0.43, 0.53] & 0.58 \\
Other's views           & 0.48 [0.43, 0.53] & 0.43 \\
\bottomrule
\end{tabular}
\caption{Spearman correlation [95\% confidence bounds] for support vector machine classifier}
\label{tab:svm_classification}
\end{table}


\subsubsection{Multi-label LSTM}
\label{section:multi_rnn}
Results for the Multi-label LSTM generally approximate the published values, with four out of nine schemas (Global self-evaluation, Health, Other people, and Other's views on self) falling within the authors' 95\% confidence bounds. The full list of results is presented in Table \ref{tab:multi_lstm} below. 

\begin{table}[H]
\centering
% you can use l,r,c for left-aligned, right-aligned or centered columns
\begin{tabular}{lcc}
\toprule
Schema                  &Published          &Reproduced \\ % use ampersands to separate columns and \\ for end of rows
\midrule
Attachment              & 0.67 [0.66, 0.72] & 0.63 \\
Competence              & 0.66 [0.64, 0.69] & 0.63 \\
Global self-eval        & 0.49 [0.45, 0.53] & 0.49 \\
Health                  & 0.35 [0.31, 0.39] & 0.35 \\
Power \& ctrl           & 0.31 [0.27, 0.34] & 0.06 \\
Meta-cognition          & 0.11 [0.06, 0.14] & 0.03 \\
Other people            & 0.16 [0.10, 0.20] & 0.11 \\
Hopelessness            & 0.53 [0.50, 0.56] & 0.44 \\
Other's views           & 0.50 [0.47, 0.54] & 0.49 \\
\bottomrule
\end{tabular}
\caption{Spearman correlation [95\% confidence bounds] for multi-label LSTM model}
\label{tab:multi_lstm}
\end{table}

Results show some variation between published data and reproduced data. These variations are attributed to a number of factors: 
\begin{itemize}
  \item A random partition of train/validation/test sets was used. \citeauthor{burger_2021} created sets of samples with equal distributions of each schema. This may explain the much lower scores we obtain for schemas that were sparsely represented in the source data, such as ``Power and Control" and ``Meta-cognition."
  \item Model architecture was implemented using Pytorch instead of TensorFlow. This includes the LSTM, loss function, optimizer, etc.
  \item Unique padding, unknown, and End of Sentence tokens were implemented. \citeauthor{burger_2021} only used padding tokens, with unknown tokens handled through TensorFlow's default functionality.
  \item The original Keras models used categorical cross entropy, which has no direct pyTorch equivalent. In its place we used CrossEntropyLoss. We also doubled the researchers' learning rate to 0.002.
\end{itemize}

\subsubsection{Per-schema LSTMs}

These models predicted the rating of each utterance against each separate schema, on the 0-3 rating scale. Our models predicted the majority class for all utterances, i.e. that the utterances were not related to any schema. This is why it scores "best" on rarely-occurring schemas like Power \& Control.

\begin{table}[H]
\centering
\begin{tabular}{lcc}
\toprule
Schema                  &Published          &Reproduced \\
\midrule
Attachment              & 0.73 [0.70, 0.76] & 0.52 \\
Competence              & 0.76 [0.72, 0.79] & 0.56 \\
Global self-eval        & 0.58 [0.54, 0.63] & 0.54 \\
Health                  & 0.75 [0.65, 0.82] & 0.69 \\
Power \& ctrl           & 0.28 [0.20, 0.35] & 0.65 \\
Meta-cognition          & -0.01 [0.00, -0.01] & 0.73 \\
Other people            & 0.22 [0.07, 0.33] & 0.72 \\
Hopelessness            & 0.63 [0.56, 0.68] & 0.65 \\
Other's views           & 0.58 [0.52, 0.63] & 0.64 \\
\bottomrule
\end{tabular}
\caption{Spearman correlation [95\% confidence bounds] for per-schema LSTM model}
\label{tab:per_schema_lstm}
\end{table}

We tried several measures to change this behavior, including:
\begin{itemize}
  \item assigning prior weights to each rating based on their inverse frequencies, to make the models less likely to predict 0
  \item removing utterances of 1-2 words, on the hypothesis that bidirectional LSTMs would not function well on such short inputs
  \item avoiding special end-of-line markers to better match the original paper's code
\end{itemize}

Unfortunately, none of these approaches improved results.

\subsection{Claim Results}


\subsection{Further studies}
\label{further_studies}
The following additional studies were conducted to explore hypotheses outside of those of the original authors. 
\subsubsection{Relaxed Preprocessing}
\label{section:relaxed_preprocessing}
RNNs combined with GLoVE word embeddings are able to capture a significant amount of information. \citeauthor{burger_2021} preprocessed their text inputs to a high degree, removing English stop words, punctuation, and contractions. We hypothesized that leaving these words in place would allow GLoVE and our multilabel RNN model access to more information, possibly making it more expressive. For example, we theorized that ``I can't" might carry some semantic difference to ``I cannot", and ``a problem" might have different meaning from ``the problem". 

We ran the model described in Section \ref{tab:multi_rnn} above, using our own preprocessing. We allowed for a slightly larger vocabulary (2100 words) to account for the inclusion of a larger vocabulary. Our preprocessing did not remove stop words, and it allowed common punctuation and contractions. 

The results of this experiment are shown in Table \ref{tab:multi_lstm} below.

\begin{table}[H]
\centering
% you can use l,r,c for left-aligned, right-aligned or centered columns
\begin{tabular}{lcc}
\toprule
Schema                  &Baseline          &Relaxed \\ % use ampersands to separate columns and \\ for end of rows
\midrule
Attachment              & 0.63 & 0.60\\
Competence              & 0.63 & 0.61\\
Global self-eval        & 0.49 & 0.43\\
Health                  & 0.35 & 0.30\\
Power \& ctrl           & 0.06 & 0.22\\
Meta-cognition          & 0.03 & 0.04\\
Other people            & 0.11 & 0.13\\
Hopelessness            & 0.44 & 0.35\\
Other's views           & 0.49 & 0.51\\
\bottomrule
\end{tabular}
\caption{Spearman correlation for baseline and relaxed preprocessing}
\label{tab:relaxed_preprocessing}
\end{table}

Overall, relaxing preprocessing constraints made little difference. Four Schemas (Power \$ Control, Meta-cognition, Other people, and Other's views) made small improvements, while five worsened. 

Stop words typically carry little meaning. We theorize that the benefit of including more information was tempered by the additional noise they introduced.

\subsubsection{Scenario-level Preprocessing}
We theorized multiple utterances from the same patient replying to the same prompt might contain information that would be useful when analyzed together.  

We ran the model described in Section \ref{tab:multi_lstm} above. We used the same preprocessing described in Section \ref{section:relaxed_preprocessing}. This time, we concatenated all utterances from each patient for each scenario. We took the mean of the schema scores for the same groups. To account for the combined utterances, we increased the maximum length our model considered from 25 to 45 tokens. 

The results of this experiment are shown in Table \ref{tab:scenario_preprocessing} below.

\begin{table}[H]
\centering
% you can use l,r,c for left-aligned, right-aligned or centered columns
\begin{tabular}{lcc}
\toprule
Schema                  &Baseline          &Scenario \\ % use ampersands to separate columns and \\ for end of rows
\midrule
Attachment              & 0.63 & 0.50 \\
Competence              & 0.63 & 0.71 \\
Global self-eval        & 0.49 & 0.09 \\
Health                  & 0.35 & 0.12 \\
Power \& ctrl           & 0.06 & 0.08 \\
Meta-cognition          & 0.03 & -0.13 \\
Other people            & 0.11 & -0.11 \\
Hopelessness            & 0.44 & -0.04 \\
Other's views           & 0.49 & 0.38 \\
\bottomrule
\end{tabular}
\caption{Spearman correlation for baseline and scenario utterances}
\label{tab:scenario_preprocessing}
\end{table}

Using scenario-level utterances resulted in significant changes to most schemas. Only one Schema (Competence) showed better results, with all others worsening. We hypothesize that this is due to the significant reduction in training samples we encountered when we combined utterances. These results are also consistent with the original authors' finding that utterance/schema correlation did not improve when considering later utterances on the same scenarios.

\section{Discussion}
\subsection{What was easy}
Thanks to \citeauthor{burger_2021}, the paper's data and codebase were well-documented and easily accessible. Acquiring the material was straightforward. 

\subsection{What was difficult}
\label{section:what_was_difficult}
We struggled to precisely replicate the authors results. We intentionally switched python packages, but we received more variance than expected. We also noted large changes in results based on hyperparameter changes, such as learning rate, sample, etc. 

Despite the paper's transparency with code and data, it proved difficult to rerun the original Jupyter notebook and replicate the paper's results. On a Windows machine, the notebook's Python code had multiple indentation errors. Attempts to fix the errors allowed us to train the multi-label RNNs, but the models made predictions that had nearly zero correlation to ground truth.

The authors' saved-to-disk models worked as expected, however.

\subsection{Recommendations for reproducibility}
\citeauthor{burger_2021} sets an excellent example for reproducibility. We recommend other authors follow this example.

\section{Communication with original authors}
The corresponding author was contacted on 5/1/2021. We offered to share our project results upon completion, and we detailed the issues we encountered rerunning the paper's Jupyter notebook (see \ref{section:what_was_difficult}). The author's response was almost immediate, requesting information about the operating system we used. She noted that one instance of trouble in the authors' original reproduction efforts may have been related to differences in machine environments.

Correspondence will be ongoing.

\bibliographystyle{acl_natbib}
\bibliography{acl2021}

%\appendix


\end{document}
